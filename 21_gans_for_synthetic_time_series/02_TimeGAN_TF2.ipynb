{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+3\">Time-series Generative Adversarial Network (TimeGAN)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from the excellent paper by Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar:  \n",
    "[Time-series Generative Adversarial Networks](https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks),  \n",
    "Neural Information Processing Systems (NeurIPS), 2019.\n",
    "\n",
    "- Last updated Date: April 24th 2020\n",
    "- [Original code](https://bitbucket.org/mvdschaar/mlforhealthlabpub/src/master/alg/timegan/) author: Jinsung Yoon (jsyoon0823@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:25.390906Z",
     "start_time": "2021-02-25T06:19:25.388162Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:26.741685Z",
     "start_time": "2021-02-25T06:19:25.394146Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import GRU, Dense, RNN, GRUCell, Input\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:26.744913Z",
     "start_time": "2021-02-25T06:19:26.742634Z"
    }
   },
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:26.765158Z",
     "start_time": "2021-02-25T06:19:26.745953Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:26.778574Z",
     "start_time": "2021-02-25T06:19:26.771222Z"
    }
   },
   "outputs": [],
   "source": [
    "results_path = Path('time_gan')\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:26.789354Z",
     "start_time": "2021-02-25T06:19:26.783080Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:26.801014Z",
     "start_time": "2021-02-25T06:19:26.794052Z"
    }
   },
   "outputs": [],
   "source": [
    "log_dir = results_path / f'experiment_{experiment:02}'\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:26.809722Z",
     "start_time": "2021-02-25T06:19:26.804347Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_store = results_path / 'TimeSeriesGAN.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:26.818716Z",
     "start_time": "2021-02-25T06:19:26.813455Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_len = 24\n",
    "n_seq = 6\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:26.826970Z",
     "start_time": "2021-02-25T06:19:26.820379Z"
    }
   },
   "outputs": [],
   "source": [
    "tickers = ['BA', 'CAT', 'DIS', 'GE', 'IBM', 'KO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:26.842483Z",
     "start_time": "2021-02-25T06:19:26.828266Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_data():\n",
    "    df = (pd.read_hdf('../data/assets.h5', 'quandl/wiki/prices')\n",
    "          .adj_close\n",
    "          .unstack('ticker')\n",
    "          .loc['2000':, tickers]\n",
    "          .dropna())\n",
    "    df.to_hdf(hdf_store, 'data/real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:31.388979Z",
     "start_time": "2021-02-25T06:19:26.843376Z"
    }
   },
   "outputs": [],
   "source": [
    "select_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:32.137439Z",
     "start_time": "2021-02-25T06:19:31.389848Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_hdf(hdf_store, 'data/real')\n",
    "axes = df.div(df.iloc[0]).plot(subplots=True,\n",
    "                               figsize=(14, 6),\n",
    "                               layout=(3, 2),\n",
    "                               title=tickers,\n",
    "                               legend=False,\n",
    "                               rot=0,\n",
    "                               lw=1, \n",
    "                               color='k')\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "plt.suptitle('Normalized Price Series')\n",
    "plt.gcf().tight_layout()\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:32.537744Z",
     "start_time": "2021-02-25T06:19:32.138473Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.clustermap(df.corr(),\n",
    "               annot=True,\n",
    "               fmt='.2f',\n",
    "               cmap=sns.diverging_palette(h_neg=20,\n",
    "                                          h_pos=220), center=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:32.542523Z",
     "start_time": "2021-02-25T06:19:32.538610Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create rolling window sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:32.556731Z",
     "start_time": "2021-02-25T06:19:32.543448Z"
    }
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(df) - seq_len):\n",
    "    data.append(scaled_data[i:i + seq_len])\n",
    "\n",
    "n_windows = len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:32.771984Z",
     "start_time": "2021-02-25T06:19:32.557971Z"
    }
   },
   "outputs": [],
   "source": [
    "real_series = (tf.data.Dataset\n",
    "               .from_tensor_slices(data)\n",
    "               .shuffle(buffer_size=n_windows)\n",
    "               .batch(batch_size))\n",
    "real_series_iter = iter(real_series.repeat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up random series generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:32.774838Z",
     "start_time": "2021-02-25T06:19:32.772880Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_random_data():\n",
    "    while True:\n",
    "        yield np.random.uniform(low=0, high=1, size=(seq_len, n_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Python generator to feed a `tf.data.Dataset` that continues to call the random number generator as long as necessary and produces the desired batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:32.817738Z",
     "start_time": "2021-02-25T06:19:32.775768Z"
    }
   },
   "outputs": [],
   "source": [
    "random_series = iter(tf.data.Dataset\n",
    "                     .from_generator(make_random_data, output_types=tf.float32)\n",
    "                     .batch(batch_size)\n",
    "                     .repeat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeGAN Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design of the TimeGAN components follows the author's sample code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:32.822509Z",
     "start_time": "2021-02-25T06:19:32.819781Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim = 24\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T21:00:46.029497Z",
     "start_time": "2020-06-11T21:00:46.027650Z"
    }
   },
   "source": [
    "## Set up logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:32.830994Z",
     "start_time": "2021-02-25T06:19:32.824307Z"
    }
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer(log_dir.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T21:01:03.536533Z",
     "start_time": "2020-06-11T21:01:03.534324Z"
    }
   },
   "source": [
    "## Input place holders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:32.847634Z",
     "start_time": "2021-02-25T06:19:32.832279Z"
    }
   },
   "outputs": [],
   "source": [
    "X = Input(shape=[seq_len, n_seq], name='RealData')\n",
    "Z = Input(shape=[seq_len, n_seq], name='RandomData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN block generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep it very simple and use a very similar architecture for all four components. For a real-world application, they should be tailored to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:32.851096Z",
     "start_time": "2021-02-25T06:19:32.848664Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_rnn(n_layers, hidden_units, output_units, name):\n",
    "    return Sequential([GRU(units=hidden_units,\n",
    "                           return_sequences=True,\n",
    "                           name=f'GRU_{i + 1}') for i in range(n_layers)] +\n",
    "                      [Dense(units=output_units,\n",
    "                             activation='sigmoid',\n",
    "                             name='OUT')], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedder & Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:32.882618Z",
     "start_time": "2021-02-25T06:19:32.853498Z"
    }
   },
   "outputs": [],
   "source": [
    "embedder = make_rnn(n_layers=3, \n",
    "                    hidden_units=hidden_dim, \n",
    "                    output_units=hidden_dim, \n",
    "                    name='Embedder')\n",
    "recovery = make_rnn(n_layers=3, \n",
    "                    hidden_units=hidden_dim, \n",
    "                    output_units=n_seq, \n",
    "                    name='Recovery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T21:02:03.263130Z",
     "start_time": "2020-06-11T21:02:03.260572Z"
    }
   },
   "source": [
    "## Generator & Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:32.906699Z",
     "start_time": "2021-02-25T06:19:32.884024Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = make_rnn(n_layers=3, \n",
    "                     hidden_units=hidden_dim, \n",
    "                     output_units=hidden_dim, \n",
    "                     name='Generator')\n",
    "discriminator = make_rnn(n_layers=3, \n",
    "                         hidden_units=hidden_dim, \n",
    "                         output_units=1, \n",
    "                         name='Discriminator')\n",
    "supervisor = make_rnn(n_layers=2, \n",
    "                      hidden_units=hidden_dim, \n",
    "                      output_units=hidden_dim, \n",
    "                      name='Supervisor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeGAN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:32.909194Z",
     "start_time": "2021-02-25T06:19:32.907611Z"
    }
   },
   "outputs": [],
   "source": [
    "train_steps = 10000\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:32.916905Z",
     "start_time": "2021-02-25T06:19:32.910053Z"
    }
   },
   "outputs": [],
   "source": [
    "mse = MeanSquaredError()\n",
    "bce = BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Autoencoder Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:34.116956Z",
     "start_time": "2021-02-25T06:19:32.917805Z"
    }
   },
   "outputs": [],
   "source": [
    "H = embedder(X)\n",
    "X_tilde = recovery(H)\n",
    "\n",
    "autoencoder = Model(inputs=X,\n",
    "                    outputs=X_tilde,\n",
    "                    name='Autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:34.121389Z",
     "start_time": "2021-02-25T06:19:34.117827Z"
    }
   },
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:34.190689Z",
     "start_time": "2021-02-25T06:19:34.122250Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(autoencoder,\n",
    "           to_file=(results_path / 'autoencoder.png').as_posix(),\n",
    "           show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:34.193740Z",
     "start_time": "2021-02-25T06:19:34.191675Z"
    }
   },
   "outputs": [],
   "source": [
    "autoencoder_optimizer = Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:19:34.202101Z",
     "start_time": "2021-02-25T06:19:34.194766Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_autoencoder_init(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_tilde = autoencoder(x)\n",
    "        embedding_loss_t0 = mse(x, x_tilde)\n",
    "        e_loss_0 = 10 * tf.sqrt(embedding_loss_t0)\n",
    "\n",
    "    var_list = embedder.trainable_variables + recovery.trainable_variables\n",
    "    gradients = tape.gradient(e_loss_0, var_list)\n",
    "    autoencoder_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return tf.sqrt(embedding_loss_t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:10.927928Z",
     "start_time": "2021-02-25T06:19:34.203159Z"
    }
   },
   "outputs": [],
   "source": [
    "for step in tqdm(range(train_steps)):\n",
    "    X_ = next(real_series_iter)\n",
    "    step_e_loss_t0 = train_autoencoder_init(X_)\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('Loss Autoencoder Init', step_e_loss_t0, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:10.930382Z",
     "start_time": "2021-02-25T06:24:10.928881Z"
    }
   },
   "outputs": [],
   "source": [
    "# autoencoder.save(log_dir / 'autoencoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Supervised training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:10.944319Z",
     "start_time": "2021-02-25T06:24:10.931170Z"
    }
   },
   "outputs": [],
   "source": [
    "supervisor_optimizer = Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:10.953796Z",
     "start_time": "2021-02-25T06:24:10.945410Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_supervisor(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = embedder(x)\n",
    "        h_hat_supervised = supervisor(h)\n",
    "        g_loss_s = mse(h[:, 1:, :], h_hat_supervised[:, :-1, :])\n",
    "\n",
    "    var_list = supervisor.trainable_variables\n",
    "    gradients = tape.gradient(g_loss_s, var_list)\n",
    "    supervisor_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return g_loss_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.241767Z",
     "start_time": "2021-02-25T06:24:10.954836Z"
    }
   },
   "outputs": [],
   "source": [
    "for step in tqdm(range(train_steps)):\n",
    "    X_ = next(real_series_iter)\n",
    "    step_g_loss_s = train_supervisor(X_)\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('Loss Generator Supervised Init', step_g_loss_s, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.243084Z",
     "start_time": "2021-02-25T06:19:25.542Z"
    }
   },
   "outputs": [],
   "source": [
    "# supervisor.save(log_dir / 'supervisor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Architecture - Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.243640Z",
     "start_time": "2021-02-25T06:19:25.547Z"
    }
   },
   "outputs": [],
   "source": [
    "E_hat = generator(Z)\n",
    "H_hat = supervisor(E_hat)\n",
    "Y_fake = discriminator(H_hat)\n",
    "\n",
    "adversarial_supervised = Model(inputs=Z,\n",
    "                               outputs=Y_fake,\n",
    "                               name='AdversarialNetSupervised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.244191Z",
     "start_time": "2021-02-25T06:19:25.550Z"
    }
   },
   "outputs": [],
   "source": [
    "adversarial_supervised.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.244764Z",
     "start_time": "2021-02-25T06:19:25.553Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(adversarial_supervised, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Architecture in Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.245320Z",
     "start_time": "2021-02-25T06:19:25.557Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_fake_e = discriminator(E_hat)\n",
    "\n",
    "adversarial_emb = Model(inputs=Z,\n",
    "                    outputs=Y_fake_e,\n",
    "                    name='AdversarialNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.245843Z",
     "start_time": "2021-02-25T06:19:25.560Z"
    }
   },
   "outputs": [],
   "source": [
    "adversarial_emb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.246312Z",
     "start_time": "2021-02-25T06:19:25.562Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(adversarial_emb, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean & Variance Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.246830Z",
     "start_time": "2021-02-25T06:19:25.567Z"
    }
   },
   "outputs": [],
   "source": [
    "X_hat = recovery(H_hat)\n",
    "synthetic_data = Model(inputs=Z,\n",
    "                       outputs=X_hat,\n",
    "                       name='SyntheticData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.247327Z",
     "start_time": "2021-02-25T06:19:25.569Z"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_data.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.247826Z",
     "start_time": "2021-02-25T06:19:25.571Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(synthetic_data, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.248328Z",
     "start_time": "2021-02-25T06:19:25.574Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_generator_moment_loss(y_true, y_pred):\n",
    "    y_true_mean, y_true_var = tf.nn.moments(x=y_true, axes=[0])\n",
    "    y_pred_mean, y_pred_var = tf.nn.moments(x=y_pred, axes=[0])\n",
    "    g_loss_mean = tf.reduce_mean(tf.abs(y_true_mean - y_pred_mean))\n",
    "    g_loss_var = tf.reduce_mean(tf.abs(tf.sqrt(y_true_var + 1e-6) - tf.sqrt(y_pred_var + 1e-6)))\n",
    "    return g_loss_mean + g_loss_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture: Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.248841Z",
     "start_time": "2021-02-25T06:19:25.579Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_real = discriminator(H)\n",
    "discriminator_model = Model(inputs=X,\n",
    "                            outputs=Y_real,\n",
    "                            name='DiscriminatorReal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.249355Z",
     "start_time": "2021-02-25T06:19:25.582Z"
    }
   },
   "outputs": [],
   "source": [
    "discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.249841Z",
     "start_time": "2021-02-25T06:19:25.585Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(discriminator_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.250714Z",
     "start_time": "2021-02-25T06:19:25.589Z"
    }
   },
   "outputs": [],
   "source": [
    "generator_optimizer = Adam()\n",
    "discriminator_optimizer = Adam()\n",
    "embedding_optimizer = Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.251539Z",
     "start_time": "2021-02-25T06:19:25.593Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_generator(x, z):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_fake = adversarial_supervised(z)\n",
    "        generator_loss_unsupervised = bce(y_true=tf.ones_like(y_fake),\n",
    "                                          y_pred=y_fake)\n",
    "\n",
    "        y_fake_e = adversarial_emb(z)\n",
    "        generator_loss_unsupervised_e = bce(y_true=tf.ones_like(y_fake_e),\n",
    "                                            y_pred=y_fake_e)\n",
    "        h = embedder(x)\n",
    "        h_hat_supervised = supervisor(h)\n",
    "        generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
    "\n",
    "        x_hat = synthetic_data(z)\n",
    "        generator_moment_loss = get_generator_moment_loss(x, x_hat)\n",
    "\n",
    "        generator_loss = (generator_loss_unsupervised +\n",
    "                          generator_loss_unsupervised_e +\n",
    "                          100 * tf.sqrt(generator_loss_supervised) +\n",
    "                          100 * generator_moment_loss)\n",
    "\n",
    "    var_list = generator.trainable_variables + supervisor.trainable_variables\n",
    "    gradients = tape.gradient(generator_loss, var_list)\n",
    "    generator_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return generator_loss_unsupervised, generator_loss_supervised, generator_moment_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.252327Z",
     "start_time": "2021-02-25T06:19:25.597Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_embedder(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = embedder(x)\n",
    "        h_hat_supervised = supervisor(h)\n",
    "        generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
    "\n",
    "        x_tilde = autoencoder(x)\n",
    "        embedding_loss_t0 = mse(x, x_tilde)\n",
    "        e_loss = 10 * tf.sqrt(embedding_loss_t0) + 0.1 * generator_loss_supervised\n",
    "\n",
    "    var_list = embedder.trainable_variables + recovery.trainable_variables\n",
    "    gradients = tape.gradient(e_loss, var_list)\n",
    "    embedding_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return tf.sqrt(embedding_loss_t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.252937Z",
     "start_time": "2021-02-25T06:19:25.600Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_discriminator_loss(x, z):\n",
    "    y_real = discriminator_model(x)\n",
    "    discriminator_loss_real = bce(y_true=tf.ones_like(y_real),\n",
    "                                  y_pred=y_real)\n",
    "\n",
    "    y_fake = adversarial_supervised(z)\n",
    "    discriminator_loss_fake = bce(y_true=tf.zeros_like(y_fake),\n",
    "                                  y_pred=y_fake)\n",
    "\n",
    "    y_fake_e = adversarial_emb(z)\n",
    "    discriminator_loss_fake_e = bce(y_true=tf.zeros_like(y_fake_e),\n",
    "                                    y_pred=y_fake_e)\n",
    "    return (discriminator_loss_real +\n",
    "            discriminator_loss_fake +\n",
    "            gamma * discriminator_loss_fake_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.253575Z",
     "start_time": "2021-02-25T06:19:25.603Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_discriminator(x, z):\n",
    "    with tf.GradientTape() as tape:\n",
    "        discriminator_loss = get_discriminator_loss(x, z)\n",
    "\n",
    "    var_list = discriminator.trainable_variables\n",
    "    gradients = tape.gradient(discriminator_loss, var_list)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return discriminator_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.254156Z",
     "start_time": "2021-02-25T06:19:25.606Z"
    }
   },
   "outputs": [],
   "source": [
    "step_g_loss_u = step_g_loss_s = step_g_loss_v = step_e_loss_t0 = step_d_loss = 0\n",
    "for step in range(train_steps):\n",
    "    # Train generator (twice as often as discriminator)\n",
    "    for kk in range(2):\n",
    "        X_ = next(real_series_iter)\n",
    "        Z_ = next(random_series)\n",
    "\n",
    "        # Train generator\n",
    "        step_g_loss_u, step_g_loss_s, step_g_loss_v = train_generator(X_, Z_)\n",
    "        # Train embedder\n",
    "        step_e_loss_t0 = train_embedder(X_)\n",
    "\n",
    "    X_ = next(real_series_iter)\n",
    "    Z_ = next(random_series)\n",
    "    step_d_loss = get_discriminator_loss(X_, Z_)\n",
    "    if step_d_loss > 0.15:\n",
    "        step_d_loss = train_discriminator(X_, Z_)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f'{step:6,.0f} | d_loss: {step_d_loss:6.4f} | g_loss_u: {step_g_loss_u:6.4f} | '\n",
    "              f'g_loss_s: {step_g_loss_s:6.4f} | g_loss_v: {step_g_loss_v:6.4f} | e_loss_t0: {step_e_loss_t0:6.4f}')\n",
    "\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('G Loss S', step_g_loss_s, step=step)\n",
    "        tf.summary.scalar('G Loss U', step_g_loss_u, step=step)\n",
    "        tf.summary.scalar('G Loss V', step_g_loss_v, step=step)\n",
    "        tf.summary.scalar('E Loss T0', step_e_loss_t0, step=step)\n",
    "        tf.summary.scalar('D Loss', step_d_loss, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist Synthetic Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.254647Z",
     "start_time": "2021-02-25T06:19:25.610Z"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_data.save(log_dir / 'synthetic_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.256838Z",
     "start_time": "2021-02-25T06:19:25.614Z"
    }
   },
   "outputs": [],
   "source": [
    "generated_data = []\n",
    "for i in range(int(n_windows / batch_size)):\n",
    "    Z_ = next(random_series)\n",
    "    d = synthetic_data(Z_)\n",
    "    generated_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.257620Z",
     "start_time": "2021-02-25T06:19:25.616Z"
    }
   },
   "outputs": [],
   "source": [
    "len(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.258015Z",
     "start_time": "2021-02-25T06:19:25.619Z"
    }
   },
   "outputs": [],
   "source": [
    "generated_data = np.array(np.vstack(generated_data))\n",
    "generated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.258527Z",
     "start_time": "2021-02-25T06:19:25.621Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save(log_dir / 'generated_data.npy', generated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.259337Z",
     "start_time": "2021-02-25T06:19:25.625Z"
    }
   },
   "outputs": [],
   "source": [
    "generated_data = (scaler.inverse_transform(generated_data\n",
    "                                           .reshape(-1, n_seq))\n",
    "                  .reshape(-1, seq_len, n_seq))\n",
    "generated_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.259932Z",
     "start_time": "2021-02-25T06:19:25.629Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore(hdf_store) as store:\n",
    "    store.put('data/synthetic', pd.DataFrame(generated_data.reshape(-1, n_seq),\n",
    "                                             columns=tickers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot sample Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:24:44.260458Z",
     "start_time": "2021-02-25T06:19:25.633Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(14, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "index = list(range(1, 25))\n",
    "synthetic = generated_data[np.random.randint(n_windows)]\n",
    "\n",
    "idx = np.random.randint(len(df) - seq_len)\n",
    "real = df.iloc[idx: idx + seq_len]\n",
    "\n",
    "for j, ticker in enumerate(tickers):\n",
    "    (pd.DataFrame({'Real': real.iloc[:, j].values,\n",
    "                   'Synthetic': synthetic[:, j]})\n",
    "     .plot(ax=axes[j],\n",
    "           title=ticker,\n",
    "           secondary_y='Synthetic', style=['-', '--'],\n",
    "           lw=1))\n",
    "sns.despine()\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
